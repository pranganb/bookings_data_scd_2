{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c76d7a-866a-43e2-9133-2f51b55b77c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, current_timestamp, sum as _sum\n",
    "from delta.tables import DeltaTable\n",
    "from pydeequ.checks import Check, CheckLevel, CheckStatus\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "import os\n",
    "\n",
    "print(os.environ['SPARK_VERSION'])\n",
    "# Get job parameters from Databricks\n",
    "date_str = dbutils.widgets.get(\"arrival_date\")\n",
    "\n",
    "\n",
    "# define file paths\n",
    "booking_data = f\"/Volumes/incremental_data_load/default/orders_data/bookings/bookings_{date_str}.csv\"\n",
    "customer_data = f\"/Volumes/incremental_data_load/default/orders_data/customer/customers_{date_str}.csv\"\n",
    "\n",
    "# read booking data\n",
    "bookings_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(booking_data)\n",
    "\n",
    "bookings_df.printSchema()\n",
    "display(bookings_df)\n",
    "\n",
    "# read customer data \n",
    "\n",
    "customer_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(customer_data)\n",
    "\n",
    "customer_df.printSchema()\n",
    "display(customer_df)\n",
    "\n",
    "# define check constraint on booking data\n",
    "\n",
    "check_incremental = Check(spark, CheckLevel.Error, \"Booking data\") \\\n",
    "    .hasSize(lambda x: x>0) \\\n",
    "    .isUnique(\"booking_id\") \\\n",
    "    .isComplete(\"customer_id\") \\\n",
    "    .isComplete(\"amount\") \\\n",
    "    .isNonNegative(\"amount\") \\\n",
    "    .isNonNegative(\"quantity\") \\\n",
    "    .isNonNegative(\"discount\")\n",
    "\n",
    "\n",
    "check_scd = Check(spark, CheckLevel.Error, \"Customer data\") \\\n",
    "    .hasSize(lambda x: x>0) \\\n",
    "    .isUnique(\"customer_id\") \\\n",
    "    .isComplete(\"customer_name\") \\\n",
    "    .isComplete(\"customer_address\") \\\n",
    "    .isComplete(\"email\")\n",
    "\n",
    "# run verification suite\n",
    "\n",
    "booking_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(bookings_df) \\\n",
    "    .addCheck(check_incremental) \\\n",
    "    .run()\n",
    "\n",
    "\n",
    "customer_dq_check = VerificationSuite(spark) \\\n",
    "    .onData(customer_df) \\\n",
    "    .addCheck(check_scd) \\\n",
    "    .run()\n",
    "\n",
    "\n",
    "booking_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, booking_dq_check)\n",
    "display(booking_dq_check_df)\n",
    "\n",
    "customer_dq_check_df = VerificationResult.checkResultsAsDataFrame(spark, customer_dq_check)\n",
    "display(customer_dq_check_df)\n",
    "\n",
    "\n",
    "if booking_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data quality check failed for booking data\")\n",
    "if customer_dq_check.status != \"Success\":\n",
    "    raise ValueError(\"Data quality check failed for customer data\")\n",
    "\n",
    "# Add ingestion timestamp to booking data\n",
    "bookings_df_incremental = bookings_df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "# join booking data with customer data\n",
    "df_joined = bookings_df_incremental.join(customer_df, \"customer_id\")\n",
    "\n",
    "# business logic: calcuate total cost after discount and filter\n",
    "\n",
    "df_transformed = df_joined \\\n",
    "    .withColumn(\"total_cost\", col(\"amount\") - col(\"discount\")) \\\n",
    "    .filter(col(\"quantity\") > 0)\n",
    "\n",
    "# Group by and aggregate\n",
    "df_transformed_agg = df_transformed \\\n",
    "    .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "    .agg(\n",
    "        _sum(\"total_cost\").alias(\"total_amount_sum\"),\n",
    "        _sum(\"quantity\").alias(\"total_quantity_sum\")\n",
    "    )\n",
    "\n",
    "# Check if the Delta table exists\n",
    "fact_table_path = \"incremental_data_load.default.booking_fact\"\n",
    "fact_table_exists = spark._jsparkSession.catalog().tableExists(fact_table_path)\n",
    "\n",
    "if fact_table_exists:\n",
    "    # read existing table\n",
    "    df_exisiting_fact = spark.read.format(\"delta\").table(fact_table_path)\n",
    "\n",
    "    # combine the aggregated data\n",
    "    df_combined = df_exisiting_fact.unionByName(df_transformed_agg, allowMissingColumns=True)\n",
    "\n",
    "     # Perform another group by and aggregation on the combined data\n",
    "    df_final_agg = df_combined \\\n",
    "        .groupBy(\"booking_type\", \"customer_id\") \\\n",
    "        .agg(\n",
    "            _sum(\"total_amount_sum\").alias(\"total_amount_sum\"),\n",
    "            _sum(\"total_quantity_sum\").alias(\"total_quantity_sum\")\n",
    "        )\n",
    "else:\n",
    "    # If the fact table doesn't exist, use the aggregated transformed data directly\n",
    "    df_final_agg = df_transformed_agg\n",
    "\n",
    "display(df_final_agg)\n",
    "\n",
    "# Write the final aggregated data back to the Delta table\n",
    "df_final_agg.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table_path)\n",
    "\n",
    "scd_table_path = \"incremental_data_load.default.customer_dim\"\n",
    "scd_table_exists = spark._jsparkSession.catalog().tableExists(scd_table_path)\n",
    "\n",
    "# Check if the customers table exists\n",
    "if scd_table_exists:\n",
    "    # Load the existing SCD table\n",
    "    scd_table = DeltaTable.forName(spark, scd_table_path)\n",
    "    display(scd_table.toDF())\n",
    "    \n",
    "    # Perform SCD2 merge logic\n",
    "    scd_table.alias(\"scd\") \\\n",
    "        .merge(\n",
    "            customer_df.alias(\"updates\"),\n",
    "            \"scd.customer_id = updates.customer_id and scd.valid_to = '9999-12-31'\"\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set={\n",
    "            \"valid_to\": \"updates.valid_from\",\n",
    "        }) \\\n",
    "        .execute()\n",
    "\n",
    "    customer_df.write.format(\"delta\").mode(\"append\").saveAsTable(scd_table_path)\n",
    "else:\n",
    "    # If the SCD table doesn't exist, write the customer data as a new Delta table\n",
    "    customer_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(scd_table_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bookings_data_with_SC2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
